# -*- coding: utf-8 -*-
"""Parametized_Job_Scraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fOLvVqufr_ZvnbdChpK4Mk_ai8KnfRk5
"""

!pip install httpx[http2]
!pip install fake-useragent

import httpx
import json
import csv
import os
import random
import time
from fake_useragent import UserAgent

# API information
API_HOST = "jsearch.p.rapidapi.com"
API_KEY = "insert api key here"

# Read job titles from a text file
def read_job_titles(file_path):
    with open(file_path, "r") as file:
        job_titles = [line.strip() for line in file.readlines()]
    return job_titles

# Read common attribute mappings from a CSV file
def read_common_attributes(file_path):
    attributes = {}
    with open(file_path, mode='r', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            common_label = row["common_attribute_label"]
            attribute_values = row["common_attribute_value"].split(',')
            attributes[common_label] = attribute_values
    return attributes

# Function to call the API and get job listings
def get_job_listings_from_api(job_title, location="USA"):
    conn = httpx.get(f"https://{API_HOST}/search?query={job_title}%20in%20{location}&page=1&num_pages=1&date_posted=all",
                     headers={
                         'x-rapidapi-key': API_KEY,
                         'x-rapidapi-host': API_HOST
                     })

    if conn.status_code == 200:
        data = conn.json()
        return data
    else:
        print(f"Failed to retrieve data from API for {job_title}. Status code: {conn.status_code}")
        return None

# Utility function to get the first non-null value based on multiple keys
def get_non_null_value(job, keys):
    """Returns the first non-null value from a list of keys in the job dictionary."""
    for key in keys:
        value = job.get(key)
        if value is not None:
            return value
    return "N/A"  # Default value if all keys are None

# Function to scrape job details and write to CSV
def scrape_jobs(url, job_titles, output_file, attributes_file):
    # Read the common attributes from the file
    common_attributes = read_common_attributes(attributes_file)

    ua = UserAgent()
    headers = {
        'User-Agent': ua.random,
        'Accept': 'application/json',
        'Referer': url,
        'Connection': 'keep-alive',
    }

    with httpx.Client(headers=headers, timeout=10.0, follow_redirects=True, http2=True) as client:
        results = []

        for job_title in job_titles:
            print(f"Searching for: {job_title}")
            job_data = get_job_listings_from_api(job_title)

            if job_data:
                for job in job_data.get('data', []):
                    job_info = {
                        common_label: get_non_null_value(job, attribute_values)
                        for common_label, attribute_values in common_attributes.items()
                    }
                    results.append(job_info)

                # Introduce a random delay to avoid triggering anti-bot measures
                delay = random.uniform(1, 5)
                print(f"Waiting for {delay:.2f} seconds before the next request...")
                time.sleep(delay)

        # Write to CSV
        with open(output_file, "w", newline='', encoding='utf-8') as csvfile:
            fieldnames = list(common_attributes.keys())
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(results)
        print(f"Job listings saved to {output_file}")

# Main function to run the scraper
def main():
    # Prompt user for input
    url = input("Enter the job site URL to scrape (e.g., https://www.monster.com): ").strip()
    domain_name = url.split("//")[-1].split("/")[0].split('.')[-2]  # Extract domain for output filename

    # Determine the attributes file to use
    attributes_file = f"job_attributes_{domain_name}.txt"
    if not os.path.exists(attributes_file):
        print(f"Attributes file '{attributes_file}' not found. Using default attributes file.")
        attributes_file = "job_attributes_default.txt"

    # Read job titles from text file
    job_titles_file = "job_titles.txt"  # Ensure this file exists with job titles listed line by line
    if not os.path.exists(job_titles_file):
        print(f"Job titles file '{job_titles_file}' not found.")
        return

    job_titles = read_job_titles(job_titles_file)

    # Generate output filename based on the domain
    output_file = f"scraped_jobs_{domain_name}.csv"

    # Start scraping
    scrape_jobs(url, job_titles, output_file, attributes_file)

if __name__ == "__main__":
    main()
